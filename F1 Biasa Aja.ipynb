{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KASDD F1 Lap time - Biasa Aja**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sumber**\n",
    "> https://www.datacamp.com/tutorial/random-forests-classifier-python <br>\n",
    "> https://www.freecodecamp.org/news/how-to-use-the-tree-based-algorithm-for-machine-learning/ <br>\n",
    "> https://forecastegy.com/posts/does-random-forest-need-feature-scaling-or-normalization/#:~:text=If%20you%20are%20using%20Random,does%20not%20require%20feature%20scaling. <br>\n",
    "> https://medium.com/@jackiee.jecksom/clustering-and-principal-component-analysis-pca-from-sklearn-c8ea5fed6648 <br>\n",
    "> https://365datascience.com/tutorials/python-tutorials/pca-k-means/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library dan data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.stats as scp\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "from decimal import Decimal\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import chi2, mutual_info_regression, mutual_info_classif, SelectKBest, mutual_info_regression, SelectPercentile, mutual_info_regression, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import SilhouetteVisualizer, KElbowVisualizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "f1_data = pd.read_csv(\"f1_train.csv\")\n",
    "f1_data = f1_data.drop(axis=1, columns=[\"ID\"])\n",
    "f1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cek_duplicates(df):\n",
    "    if df.duplicated().sum() > 0:\n",
    "        print(\"Terdapat\", df.duplicated().sum(), \"pasang data yang redundan\")\n",
    "        display(df[df.duplicated()])\n",
    "    else:\n",
    "        print(\"Tidak ada data yang redundan\")\n",
    "\n",
    "def cek_null(df):\n",
    "    col_na = df.isnull().sum().sort_values(ascending=True)\n",
    "    percent = col_na*100 / len(df)\n",
    "\n",
    "    missing_data = pd.concat([col_na, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "    if (missing_data[missing_data['Total'] > 0].shape[0] == 0):\n",
    "        print(\"Tidak ditemukan missing value pada dataset\")\n",
    "\n",
    "    else:\n",
    "        print(missing_data[missing_data['Total'] > 0])\n",
    "\n",
    "def cek_outlier(df):\n",
    "    df_numerical = df.select_dtypes(include=['float64', 'int64']) \n",
    "    Q1 = df_numerical.quantile(0.25, numeric_only=True)\n",
    "    Q3 = df_numerical.quantile(0.75, numeric_only=True)\n",
    "\n",
    "    # Menghitung RUB dan RLB.\n",
    "    IQR = Q3 - Q1\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Menampilkan banyaknya outlier pada atribut.\n",
    "    outliers = (df_numerical < lower_limit) | (df_numerical > upper_limit)\n",
    "\n",
    "    # Menghitung dan menampilkan persentase outlier pada tiap atribut.\n",
    "    percentage_outliers = (outliers.sum() / len(df)) * 100\n",
    "    print(\"Persentase Outlier pada tiap atribut:\")\n",
    "    print(percentage_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek_null(f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Atribut `PitOutTime`, `PitInTime`, dan `DeletedReason` memiliki persentase jumlah missing value mencapai lebih dari 90%. Oleh karena itu, atribut-atribut tersebut perlu di drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data = f1_data.drop(axis=1, columns=['PitOutTime', 'PitInTime', 'DeletedReason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_type_and_distribution(df):\n",
    "    print(\"Menampilkan informasi dataset dan tipe data\")\n",
    "    print('#'*50)\n",
    "    df.info()\n",
    "    print('#'*50)\n",
    "    print(\"Menampilkan distribusi data numerik\")\n",
    "    numerics = ['SpeedI2', 'SpeedFL', 'SpeedST', 'SpeedI1']\n",
    "    for col in numerics:\n",
    "        df_feature = f1_data[col]\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title(f'Distribusi data {col}')\n",
    "        sns.kdeplot(df_feature, fill=True)\n",
    "        plt.show()\n",
    "    print(\"Menampilkan Modus dari data kategorikal\")\n",
    "    categoricals = [\"IsPersonalBest\", \"Sector2SessionTime\", \"Sector2Time\",\"Sector3SessionTime\", \"Sector3Time\", \"LapTime\", \"Sector1Time\", \"Sector1SessionTime\"]\n",
    "    for col in categoricals:\n",
    "        print(f\"Mode for {col} = {f1_data[col].mode()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data_type_and_distribution(f1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_mean(df):\n",
    "    mean = df.mean()\n",
    "    filled_df = df.fillna(mean)\n",
    "    return filled_df\n",
    "def fill_null_median(df):\n",
    "    median = df.median()\n",
    "    filled_df = df.fillna(median)\n",
    "    return filled_df\n",
    "def fill_null_mode(df):\n",
    "    mode = df.mode()[0]\n",
    "    filled_df = df.fillna(mode)\n",
    "    return filled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data['SpeedI2'] = fill_null_median(f1_data['SpeedI2'])\n",
    "f1_data['SpeedFL'] = fill_null_median(f1_data['SpeedFL'])\n",
    "f1_data['SpeedST'] = fill_null_mean(f1_data['SpeedST'])\n",
    "f1_data['SpeedI1'] = fill_null_median(f1_data['SpeedI1'])\n",
    "f1_data['IsPersonalBest'] = fill_null_mode(f1_data['IsPersonalBest'])\n",
    "f1_data['Sector2SessionTime'] = fill_null_mode(f1_data['Sector2SessionTime'])\n",
    "f1_data['Sector2Time'] = fill_null_mode(f1_data['Sector2Time'])\n",
    "f1_data['Sector3SessionTime'] = fill_null_mode(f1_data['Sector3SessionTime'])\n",
    "f1_data['Sector3Time'] = fill_null_mode(f1_data['Sector3Time'])\n",
    "f1_data['LapTime'] = fill_null_mode(f1_data['LapTime'])\n",
    "f1_data['Sector1Time'] = fill_null_mode(f1_data['Sector1Time'])\n",
    "f1_data['Sector1SessionTime'] = fill_null_mode(f1_data['Sector1SessionTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek_null(f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicate Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jumlah duplikasi data : \" + str(f1_data.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data = f1_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data.boxplot(vert=False,figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptime_numerical = f1_data.select_dtypes(include=['float64', 'int64']) \n",
    "Q1 = laptime_numerical.quantile(0.25)\n",
    "Q3 = laptime_numerical.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cek_outlier(f1_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Atribut `Presure` digunakan untuk memprediksi `TyreLife`, sehingga tidak kami drop ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Menampilkan nilai unique yang terdapat pada setiap kolom kategorikal\")\n",
    "print('#'*70)\n",
    "print()\n",
    "for col in f1_data.select_dtypes(include=object).columns:\n",
    "    print(col, f\": {len(f1_data[col].unique())}\", f1_data[col].unique())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data['Compound'] = f1_data['Compound'].map({'INTERMEDIATE':1, 'MEDIUM':4, 'HARD':2, 'SOFT':3, 'WET':0})\n",
    "f1_data['Pos_cat'] = f1_data['Pos_cat'].map({'Participant':0, 'Podium':2, 'Point':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_days_remover (duration):\n",
    "    return duration.replace('0 days ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_att = []\n",
    "for col in f1_data.select_dtypes(include=object).columns:\n",
    "    time_att.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in time_att:\n",
    "    f1_data[time] = f1_data[time].apply(zero_days_remover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk memudahkan encoding waktu, kami menghilangkan 0 days karena seluruh data memiliki 0 days dan hal tersebut tidak membantu kami dalam melakukan encoding waktu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_converter (time_str):\n",
    "    if '.' in time_str:\n",
    "        time_str, milliseconds = time_str.split('.')\n",
    "        milliseconds = int(milliseconds)\n",
    "    else:\n",
    "        milliseconds = 0\n",
    "\n",
    "    x_time = datetime.datetime.strptime(time_str, '%H:%M:%S')\n",
    "\n",
    "    total_seconds = datetime.timedelta(\n",
    "        hours=x_time.hour,\n",
    "        minutes=x_time.minute,\n",
    "        seconds=x_time.second,\n",
    "        microseconds=milliseconds / 1000  # Convert milliseconds to microseconds\n",
    "    ).total_seconds()\n",
    "    \n",
    "    return total_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in time_att:\n",
    "    f1_data[time] = f1_data[time].apply(time_converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = ['IsPersonalBest', 'FreshTyre', 'Deleted', 'Rainfall']\n",
    "f1_data = pd.get_dummies(f1_data, columns = boolean_columns, drop_first=True) \n",
    "f1_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EKSPLORASI** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apakah penggunaan ‘Compound’ yang berbeda berpengaruh terhadap performa? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performa dari suatu mobil F1 dapat ditentukan melalui waktu yang ditempuh mobil tersebut selama satu lap, yaitu atribut LapTime. Atribut ini juga kami pilih sebagai representasi performa yang dianalisis pengaruhnya oleh Compound karena dalam balapan F1, tipe compound yang berbeda tidak mungkin dipakai dalam satu lap yang sama. Oleh karena itu pada eksplorasi ini kami hanya mengambil atribut Compound dan LapTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compount_influence = f1_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compount_influence = compount_influence[['Compound', 'LapTime']]\n",
    "compount_influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compount_influence['Compound'] = compount_influence['Compound'].map({1: 'INTERMEDIATE', 4: 'MEDIUM', 2: 'HARD', 3: 'SOFT', 0: 'WET'})\n",
    "compount_influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = compount_influence['Compound'].unique()\n",
    "for compound in compounds:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    subset = compount_influence[compount_influence['Compound'] == compound]['LapTime']\n",
    "    sns.kdeplot(subset, fill=True)\n",
    "    plt.title(f'KDE of Lap Times for {compound} Compound')\n",
    "    plt.xlabel('Lap Time')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_laptimes = compount_influence.groupby('Compound')['LapTime'].median()\n",
    "median_laptimes = median_laptimes.sort_values()\n",
    "median_laptimes.plot(kind='bar', figsize=(10, 6), color='skyblue')\n",
    "plt.title('Median Lap Times by Compound')\n",
    "plt.xlabel('Compound Type')\n",
    "plt.ylabel('Median Lap Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Grafik diatas adalah grafik yang menampilkan median LapTime dari setiap Compound dan diurutkan dari LapTime paling cepat hingga LapTime paling lambat\n",
    "\n",
    "> Berdasarkan visualisasi diatas, dapat dianalisis bahwa setiap tipe Compound dapat mempengaruhi performa mobil F1 karena menghasilkan LapTime yang berbeda-beda. Performa terbaik didapatkan ketika mobil-mobil F1 menggunakan Compound bertipe SOFT. Lalu, diikuti dengan tipe Compound HARD, MEDIUM, & INTERMEDIATE untuk performa terbaik kedua, ketiga, dan keempat. Untuk performa terburuk, didapatkan ketika mobil F1 menggunakan tipe Compound WET."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagaimana ciri-ciri driver dengan kategori posisi ‘Pos_cat’ Podium dibandingkan dengan kategori posisi lainnya?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis perbedaan ketika sirkuit hujan ‘Rainfall’ atau tidak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rain = f1_data.copy()\n",
    "mean_attributes = ['LapTime', 'TyreLife', 'AirTemp', 'Humidity', 'TrackTemp','WindSpeed']\n",
    "mode_attributes = ['Compound']\n",
    "\n",
    "reverse_compound_mapping = {1: 'INTERMEDIATE', 4: 'MEDIUM', 2: 'HARD', 3: 'SOFT', 0: 'WET'}\n",
    "df_rain['Compound'] = df_rain['Compound'].map(reverse_compound_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall_data = df_rain[df_rain['Rainfall_True'] == True]\n",
    "rainfall_mean = rainfall_data[mean_attributes].median()\n",
    "rainfall_mode = rainfall_data[mode_attributes].mode().iloc[0]\n",
    "\n",
    "rainfall_combined = pd.concat([rainfall_mean, rainfall_mode])\n",
    "rainfall_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rainfall_data = df_rain[df_rain['Rainfall_True'] == False]\n",
    "no_rainfall_mean = no_rainfall_data[mean_attributes].median()\n",
    "no_rainfall_mode = no_rainfall_data[mode_attributes].mode().iloc[0]\n",
    "\n",
    "no_rainfall_combined = pd.concat([no_rainfall_mean, no_rainfall_mode])\n",
    "no_rainfall_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di sini, kami akan melakukan perbandingan kondisi balapan ketika sirkuit 'Rainfall' dan tidak dengan asumsi bahwa data ini adalah kumpulan dari balapan-balapan satu musim di lokasi dan waktu yg berbeda. Dari data yang telah diolah, kami menemukan bahwa terdapat beberapa aspek yang berbeda dari sirkuit 'Rainfall' dan tidak.  LapTime dari dua keadaan sirkuit ini berbeda cukup signifikan, di mana dalam keadaan 'Rainfall' diperlukan lebih banyak waktu untuk melakukan 'LapTime' jika dibandingkan dengan track tidak 'Rainfall'. Dari segi 'AirTemp', sirkuit 'Rainfall' memiliki temperatur udara yang lebih rendah. Selain itu, sirkuit 'Rainfall' memiliki temperatur track yang lebih rendah, serta memiliki 'humidity' lebih tinggi jika dibanding track yang tidak 'Rainfall'. Ban Intermediate menjadi ban yang sering dipakai pada sirkuit 'Rainfall'. Di sirkuit yang tidak 'Rainfall', temperatur track lebih tinggi dibanding track 'Rainfall' dan memiliki 'WindDirection' lebih tinggi dibanding track 'Rainfall'. Adapun 'Compound' ban yang sering dipakai untuk sirkuit tidak 'Rainfall' adalah ban berjenis Hard. Kedua tipe sirkuit ini tidak berbeda terlalu jauh jika kita tinjau dari segi 'Pressure' atau tekanan udara."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adakah rentang umur ban ‘TyreLife’ dengan performa terbaik dibandingkan rentang umur ban lainnya?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **REGRESI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/evaluation-metrics-for-regression-models-c91c65d73af\n",
    "def regression_metrics(prediction, y_test):\n",
    "    MAE = mean_absolute_error(y_test, prediction)\n",
    "    MSE = mean_squared_error(y_test, prediction)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    R_squared = r2_score(y_test, prediction)\n",
    "\n",
    "    print('MAE: ' + str(MAE))\n",
    "    print('MSE: ' + str(MSE))\n",
    "    print('RMSE: ' + str(RMSE))\n",
    "    print('R_squared: ' + str(R_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dan seleksi fitur Dataset Klasifikasi\n",
    "X_regression = f1_data.drop(columns=['TyreLife'], axis=1)\n",
    "y_regression = f1_data['TyreLife']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regress_train, X_regress_test, y_regress_train, y_regress_test = train_test_split(X_regression, y_regression, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarisasi\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_regress_train = scaler.fit_transform(X_regress_train)\n",
    "X_regress_test = scaler.transform(X_regress_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_f1 = RandomForestRegressor()\n",
    "rf_f1.fit(X_regress_train, y_regress_train)\n",
    "\n",
    "# Memprediksi data testing\n",
    "predicted = rf_f1.predict(X_regress_test)\n",
    "regression_metrics(predicted, y_regress_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_f1 = [0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model_f1 = []\n",
    "for alpha in alpha_f1:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_regress_train, y_regress_train)\n",
    "    ridge_model_f1.append(ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics_list = []\n",
    "counter = 1\n",
    "for model in ridge_model_f1:\n",
    "    y_pred = model.predict(X_regress_test)\n",
    "    mse = metrics.mean_squared_error(y_regress_test, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_regress_test, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = metrics.r2_score(y_regress_test, y_pred)\n",
    "    \n",
    "    metrics_list.append((alpha_f1[counter-1], mse, mae, rmse, r2))\n",
    "    \n",
    "    print(f'Model Ridge regression ke {counter}')\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"R-squared:\", r2)\n",
    "    print()\n",
    "    counter += 1\n",
    "\n",
    "# Find the best model based on RMSE (you can change the criteria if needed)\n",
    "best_model_idx = max(range(len(metrics_list)), key=lambda i: metrics_list[i][4])  # Using R-squared for selection\n",
    "best_model = ridge_model_f1[best_model_idx]\n",
    "\n",
    "print(f'Model terbaik adalah model ke-{best_model_idx + 1} dengan alpha={metrics_list[best_model_idx][0]}')\n",
    "print(f'Metrik Evaluasi - MSE: {metrics_list[best_model_idx][1]}, MAE: {metrics_list[best_model_idx][2]}, RMSE: {metrics_list[best_model_idx][3]}, R-squared: {metrics_list[best_model_idx][4]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KLASIFIKASI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset Klasifikasi\n",
    "X_classification = f1_data.drop(columns=['Pos_cat'], axis=1)\n",
    "y_classification = f1_data['Pos_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_classification, y_classification, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarisasi\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_class_train = scaler.fit_transform(X_class_train)\n",
    "X_class_test = scaler.transform(X_class_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree  \n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "def classification_metrics(prediction, y_test):\n",
    "    print(f'Accuracy: {accuracy_score(y_test, prediction)}')\n",
    "    print('F1 Macro Average:', f1_score(y_test, prediction, average='macro'))\n",
    "    print('F1 Micro Average:', f1_score(y_test, prediction, average='micro'))\n",
    "    print('Precision Macro Average:', precision_score(y_test, prediction, average='macro',zero_division=0))\n",
    "    print('Precision Micro Average:', precision_score(y_test, prediction, average='micro',zero_division=0))\n",
    "    print('Recall Macro Average:', recall_score(y_test, prediction, average='macro',zero_division=0))\n",
    "    print('Recall Micro Average:', recall_score(y_test, prediction, average='micro',zero_division=0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_f1 = RandomForestClassifier()\n",
    "rf_f1.fit(X_class_train, y_class_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memprediksi data testing\n",
    "predicted = rf_f1.predict(X_class_test)\n",
    "classification_metrics(predicted, y_class_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "df_f1_pca = f1_data.copy()\n",
    "df_f1_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "f1_pca_std = scaler.fit_transform(df_f1_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(f1_pca_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1, 28), pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.xticks(range(1, 28)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 8)\n",
    "pca.fit(f1_pca_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pca = pca.transform(df_f1_pca)\n",
    "scores_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WCSS = []\n",
    "for i in range(1,28):\n",
    "  kmeans_pca = KMeans(n_clusters = i, init = \"k-means++\", random_state = 42)\n",
    "  kmeans_pca.fit(scores_pca)\n",
    "  WCSS.append(kmeans_pca.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,12))\n",
    "plt.plot(range(1,28), WCSS, marker = \"o\", linestyle = \"--\")\n",
    "plt.grid()\n",
    "plt.title(\"Cluster using PCA Scores\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.xlabel(\"N Clusters\")\n",
    "plt.xticks(range(1, 28))  # Setting x-axis ticks to show every component number\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca = KMeans(n_clusters = 3, init = \"k-means++\", random_state = 42)\n",
    "kmeans_pca.fit(scores_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new data frame with the original features and add the PCA scores and assigned clusters.\n",
    "df_segm_pca_kmeans = pd.concat([df_f1_pca.reset_index(drop=True), pd.DataFrame(scores_pca)], axis=1)\n",
    "df_segm_pca_kmeans.columns.values[-8:] = ['Component 1', 'Component 2', 'Component 3', 'Component 4', 'Component 5', 'Component 6', 'Component 7', 'Component 8']\n",
    "# The last column we add contains the pca k-means clustering labels.\n",
    "df_segm_pca_kmeans['Segment K-means PCA'] = kmeans_pca.labels_\n",
    "\n",
    "df_segm_pca_kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segm_pca_kmeans['Segment'] = df_segm_pca_kmeans['Segment K-means PCA'].map({0: 'first', 1: 'second', 2: 'third'})\n",
    "df_segm_pca_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn's pairplot to visualize relationships between each pair of components\n",
    "sns.pairplot(df_segm_pca_kmeans, vars=['Component 1', 'Component 2', 'Component 3', 'Component 4', \n",
    "                                       'Component 5', 'Component 6', 'Component 7', 'Component 8'], \n",
    "             hue='Segment', palette='viridis')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
