{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KASDD F1 Lap time - Biasa Aja**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library dan data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.stats as scp\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "from decimal import Decimal\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import chi2, mutual_info_regression, mutual_info_classif, SelectKBest, mutual_info_regression, SelectPercentile, mutual_info_regression, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import SilhouetteVisualizer, KElbowVisualizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "f1_data = pd.read_csv(\"f1_train.csv\")\n",
    "f1_data = f1_data.drop(axis=1, columns=[\"ID\"])\n",
    "f1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cek_duplicates(df):\n",
    "    if df.duplicated().sum() > 0:\n",
    "        print(\"Terdapat\", df.duplicated().sum(), \"pasang data yang redundan\")\n",
    "        display(df[df.duplicated()])\n",
    "    else:\n",
    "        print(\"Tidak ada data yang redundan\")\n",
    "\n",
    "def cek_null(df):\n",
    "    col_na = df.isnull().sum().sort_values(ascending=True)\n",
    "    percent = col_na*100 / len(df)\n",
    "\n",
    "    missing_data = pd.concat([col_na, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "    if (missing_data[missing_data['Total'] > 0].shape[0] == 0):\n",
    "        print(\"Tidak ditemukan missing value pada dataset\")\n",
    "\n",
    "    else:\n",
    "        print(missing_data[missing_data['Total'] > 0])\n",
    "\n",
    "def cek_outlier(df):\n",
    "    df_numerical = df.select_dtypes(include=['float64', 'int64']) \n",
    "    Q1 = df_numerical.quantile(0.25, numeric_only=True)\n",
    "    Q3 = df_numerical.quantile(0.75, numeric_only=True)\n",
    "\n",
    "    # Menghitung RUB dan RLB.\n",
    "    IQR = Q3 - Q1\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Menampilkan banyaknya outlier pada atribut.\n",
    "    outliers = (df_numerical < lower_limit) | (df_numerical > upper_limit)\n",
    "\n",
    "    # Menghitung dan menampilkan persentase outlier pada tiap atribut.\n",
    "    percentage_outliers = (outliers.sum() / len(df)) * 100\n",
    "    print(\"Persentase Outlier pada tiap atribut:\")\n",
    "    print(percentage_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek_null(f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Atribut `PitOutTime`, `PitInTime`, dan `DeletedReason` memiliki persentase jumlah missing value mencapai lebih dari 90%. Oleh karena itu, atribut-atribut tersebut perlu di drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data = f1_data.drop(axis=1, columns=['PitOutTime', 'PitInTime', 'DeletedReason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_type_and_distribution(df):\n",
    "    print(\"Menampilkan informasi dataset dan tipe data\")\n",
    "    print('#'*50)\n",
    "    df.info()\n",
    "    print('#'*50)\n",
    "    print(\"Menampilkan distribusi data numerik\")\n",
    "    numerics = ['SpeedI2', 'SpeedFL', 'SpeedST', 'SpeedI1']\n",
    "    for col in numerics:\n",
    "        df_feature = f1_data[col]\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title(f'Distribusi data {col}')\n",
    "        sns.kdeplot(df_feature, fill=True)\n",
    "        plt.show()\n",
    "    print(\"Menampilkan Modus dari data kategorikal\")\n",
    "    categoricals = [\"IsPersonalBest\", \"Sector2SessionTime\", \"Sector2Time\",\"Sector3SessionTime\", \"Sector3Time\", \"LapTime\", \"Sector1Time\", \"Sector1SessionTime\"]\n",
    "    for col in categoricals:\n",
    "        print(f\"Mode for {col} = {f1_data[col].mode()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data_type_and_distribution(f1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_mean(df):\n",
    "    mean = df.mean()\n",
    "    filled_df = df.fillna(mean)\n",
    "    return filled_df\n",
    "def fill_null_median(df):\n",
    "    median = df.median()\n",
    "    filled_df = df.fillna(median)\n",
    "    return filled_df\n",
    "def fill_null_mode(df):\n",
    "    mode = df.mode()[0]\n",
    "    filled_df = df.fillna(mode)\n",
    "    return filled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data['SpeedI2'] = fill_null_median(f1_data['SpeedI2'])\n",
    "f1_data['SpeedFL'] = fill_null_median(f1_data['SpeedFL'])\n",
    "f1_data['SpeedST'] = fill_null_mean(f1_data['SpeedST'])\n",
    "f1_data['SpeedI1'] = fill_null_median(f1_data['SpeedI1'])\n",
    "f1_data['IsPersonalBest'] = fill_null_mode(f1_data['IsPersonalBest'])\n",
    "f1_data['Sector2SessionTime'] = fill_null_mode(f1_data['Sector2SessionTime'])\n",
    "f1_data['Sector2Time'] = fill_null_mode(f1_data['Sector2Time'])\n",
    "f1_data['Sector3SessionTime'] = fill_null_mode(f1_data['Sector3SessionTime'])\n",
    "f1_data['Sector3Time'] = fill_null_mode(f1_data['Sector3Time'])\n",
    "f1_data['LapTime'] = fill_null_mode(f1_data['LapTime'])\n",
    "f1_data['Sector1Time'] = fill_null_mode(f1_data['Sector1Time'])\n",
    "f1_data['Sector1SessionTime'] = fill_null_mode(f1_data['Sector1SessionTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek_null(f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicate Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jumlah duplikasi data : \" + str(f1_data.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data = f1_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data.boxplot(vert=False,figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptime_numerical = f1_data.select_dtypes(include=['float64', 'int64']) \n",
    "Q1 = laptime_numerical.quantile(0.25)\n",
    "Q3 = laptime_numerical.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cek_outlier(f1_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Atribut `Presure` digunakan untuk memprediksi `TyreLife`, sehingga tidak kami drop ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Menampilkan nilai unique yang terdapat pada setiap kolom kategorikal\")\n",
    "print('#'*70)\n",
    "print()\n",
    "for col in f1_data.select_dtypes(include=object).columns:\n",
    "    print(col, f\": {len(f1_data[col].unique())}\", f1_data[col].unique())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data['Compound'] = f1_data['Compound'].map({'INTERMEDIATE':1, 'MEDIUM':4, 'HARD':2, 'SOFT':3, 'WET':0})\n",
    "f1_data['Pos_cat'] = f1_data['Pos_cat'].map({'Participant':0, 'Podium':2, 'Point':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_days_remover (duration):\n",
    "    return duration.replace('0 days ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_att = []\n",
    "for col in f1_data.select_dtypes(include=object).columns:\n",
    "    time_att.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in time_att:\n",
    "    f1_data[time] = f1_data[time].apply(zero_days_remover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk memudahkan encoding waktu, kami menghilangkan 0 days karena seluruh data memiliki 0 days dan hal tersebut tidak membantu kami dalam melakukan encoding waktu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_converter (time_str):\n",
    "    if '.' in time_str:\n",
    "        time_str, milliseconds = time_str.split('.')\n",
    "        milliseconds = int(milliseconds)\n",
    "    else:\n",
    "        milliseconds = 0\n",
    "\n",
    "    x_time = datetime.datetime.strptime(time_str, '%H:%M:%S')\n",
    "\n",
    "    total_seconds = datetime.timedelta(\n",
    "        hours=x_time.hour,\n",
    "        minutes=x_time.minute,\n",
    "        seconds=x_time.second,\n",
    "        microseconds=milliseconds / 1000  # Convert milliseconds to microseconds\n",
    "    ).total_seconds()\n",
    "    \n",
    "    return total_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in time_att:\n",
    "    f1_data[time] = f1_data[time].apply(time_converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EKSPLORASI** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apakah penggunaan ‘Compound’ yang berbeda berpengaruh terhadap performa? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compount_influence = f1_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compount_influence = compount_influence[['Compound', 'LapTime']]\n",
    "compount_influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagaimana ciri-ciri driver dengan kategori posisi ‘Pos_cat’ Podium dibandingkan dengan kategori posisi lainnya?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis perbedaan ketika sirkuit hujan ‘Rainfall’ atau tidak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rain = f1_data.copy()\n",
    "\n",
    "mean_attributes = ['LapTime', 'TyreLife', 'AirTemp', 'Humidity', 'TrackTemp','WindSpeed']\n",
    "mode_attributes = ['Compound']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall_data = df_rain[df_rain['Rainfall'] == True]\n",
    "rainfall_mean = rainfall_data[mean_attributes].median()\n",
    "rainfall_mode = rainfall_data[mode_attributes].mode().iloc[0]\n",
    "\n",
    "rainfall_combined = pd.concat([rainfall_mean, rainfall_mode])\n",
    "rainfall_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rainfall_data = df_rain[df_rain['Rainfall'] == False]\n",
    "no_rainfall_mean = no_rainfall_data[mean_attributes].median()\n",
    "no_rainfall_mode = no_rainfall_data[mode_attributes].mode().iloc[0]\n",
    "\n",
    "no_rainfall_combined = pd.concat([no_rainfall_mean, no_rainfall_mode])\n",
    "no_rainfall_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di sini, kami akan melakukan perbandingan kondisi balapan ketika sirkuit 'Rainfall' dan tidak dengan asumsi bahwa data ini adalah kumpulan dari balapan-balapan satu musim di lokasi dan waktu yg berbeda. Dari data yang telah diolah, kami menemukan bahwa terdapat beberapa aspek yang berbeda dari sirkuit 'Rainfall' dan tidak.  LapTime dari dua keadaan sirkuit ini berbeda cukup signifikan, di mana dalam keadaan 'Rainfall' diperlukan lebih banyak waktu untuk melakukan 'LapTime' jika dibandingkan dengan track tidak 'Rainfall'. Dari segi 'AirTemp', sirkuit 'Rainfall' memiliki temperatur udara yang lebih rendah. Selain itu, sirkuit 'Rainfall' memiliki temperatur track yang lebih rendah, serta memiliki 'humidity' lebih tinggi jika dibanding track yang tidak 'Rainfall'. Ban Intermediate menjadi ban yang sering dipakai pada sirkuit 'Rainfall'. Di sirkuit yang tidak 'Rainfall', temperatur track lebih tinggi dibanding track 'Rainfall' dan memiliki 'WindDirection' lebih tinggi dibanding track 'Rainfall'. Adapun 'Compound' ban yang sering dipakai untuk sirkuit tidak 'Rainfall' adalah ban berjenis Hard. Kedua tipe sirkuit ini tidak berbeda terlalu jauh jika kita tinjau dari segi 'Pressure' atau tekanan udara."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adakah rentang umur ban ‘TyreLife’ dengan performa terbaik dibandingkan rentang umur ban lainnya?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **REGRESI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KLASIFIKASI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasifikasi digunakan untuk membuat model dengan target `Pos_cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Hyperparameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
